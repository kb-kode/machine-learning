{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\"\"\"\n",
    "AirBnB Rio de Janeiro Data Challenge\n",
    "Python 3.7\n",
    "\"\"\"\n",
    "# Import\n",
    "import os\n",
    "import logging\n",
    "import operator\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "#plotting packages\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#stats packages\n",
    "import statsmodels.api as sm\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "# Functions\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Function:\n",
    "        - reads data\n",
    "        - cleans data\n",
    "        - calculates and appends supporting features\n",
    "        - run analysis and output graphs as collection of PDFs\n",
    "        - run model prediction\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Develop dataset for analysis and model prediction. Save file.\n",
    "    \n",
    "    file_path = './data/'\n",
    "    \n",
    "    print ('Cleaning and enhancing features...')\n",
    "    data = generate_features(file_path)\n",
    "    data.to_csv(file_path + 'df_cleaned.csv')\n",
    "    print ('Cleaning and enhancing COMPLETED, file created.')\n",
    "    \n",
    "    #Generate EDA Plots\n",
    "    eda_plots(data)\n",
    "    #Create dataframe for model\n",
    "    prep_model_df(data)\n",
    "    #Run RF model\n",
    "    auc, feat_imp = run_model(model_df, test_size)\n",
    "    print ('AUC: ' + str(auc))\n",
    "    print (feat_imp)\n",
    "\n",
    "def generate_features(file_path):\n",
    "    \"\"\"\n",
    "    *Return: DataFrame of Cleaned+Enhanced dataset.\n",
    "    file_path: directory where csv files are stored (string) \n",
    "    \"\"\"\n",
    "    \n",
    "    def convert_to_datetime(df, col):\n",
    "        df[col] = np.where(df[col].notnull(),df[col],\"\")\n",
    "        df[col] = [x.split('.',1)[0] for x in df[col]]\n",
    "        df[col] = df[col].apply(lambda x: datetime.strptime(x, '%Y-%m-%d %H:%M:%S') if x != '' else '')\n",
    "    \n",
    "    logging.info('Importing data from: %s' % (file_path))\n",
    "\n",
    "    listings = pd.read_csv(os.path.join(file_path, 'listings.csv'))\n",
    "    isinstance(listings, pd.DataFrame)\n",
    "    users = pd.read_csv(os.path.join(file_path, 'users.csv'))\n",
    "    isinstance(users, pd.DataFrame)\n",
    "    contacts = pd.read_csv(os.path.join(file_path, 'contacts.csv'))\n",
    "    isinstance(contacts, pd.DataFrame)\n",
    "    \n",
    "    n_samples, n_features = listings.shape\n",
    "    logging.info('Listing    # of Observations: %s' % (n_samples))\n",
    "    logging.info('Listing    # of Features: %s' % (n_features))\n",
    "    n_samples, n_features = users.shape\n",
    "    logging.info('Users    # of Observations: %s' % (n_samples))\n",
    "    logging.info('Users    # of Features: %s' % (n_features))\n",
    "    n_samples, n_features = contacts.shape\n",
    "    logging.info('Contacts    # of Observations: %s' % (n_samples))\n",
    "    logging.info('Contacts    # of Features: %s' % (n_features))\n",
    "\n",
    "    #Convert timestamps to datetime in Contacts\n",
    "    logging.info('Convert timestamps to datetime in Contacts table')\n",
    "    for col_ts in ['ts_interaction_first','ts_reply_at_first','ts_accepted_at_first','ts_booking_at']:\n",
    "        convert_to_datetime(contacts, col_ts)\n",
    "    contacts['ds_checkin_first'] = contacts['ds_checkin_first'].apply(lambda x: datetime.strptime(x, '%Y-%m-%d'))\n",
    "    contacts['ds_checkout_first'] = contacts['ds_checkout_first'].apply(lambda x: datetime.strptime(x, '%Y-%m-%d'))  \n",
    "    \n",
    "    #Length of time a host takes to reply to an inquiry (minutes)\n",
    "    logging.info('Adding Feature: reply_inq_mins: Length of time(minutes) host replies to inquiry')\n",
    "    contacts['reply_inq_mins'] = (contacts['ts_reply_at_first']-contacts['ts_interaction_first']).apply(lambda x: x.total_seconds())/60\n",
    "\n",
    "    #Length of time for host to accept an inquiry (minutes)\n",
    "    logging.info('Adding Feature: accept_inq_mins: Length of time(minutes) host takes to accept an inquiry')\n",
    "    contacts['accept_inq_mins'] = (contacts['ts_accepted_at_first']-contacts['ts_interaction_first']).apply(lambda x: x.total_seconds())/60\n",
    "\n",
    "    #Length of time for host to accept booking from inquiry(minutes)\n",
    "    logging.info('Adding Feature: book_inq_mins: Length of time(minutes) to confirm booking from inquiry')\n",
    "    contacts['book_inq_mins'] = (contacts['ts_booking_at']-contacts['ts_interaction_first']).apply(lambda x: x.total_seconds())/60\n",
    "\n",
    "    #identifier that the contact lead to a booking (0=failed, 1=succeeded)\n",
    "    logging.info('Adding Feature: booked: User successfully booked an Airbnb')\n",
    "    bookings = []\n",
    "    for book in contacts['ts_booking_at']:\n",
    "        if book is pd.Timestamp('NaT'):\n",
    "            bookings.append(0)\n",
    "        else:\n",
    "            bookings.append(1)\n",
    "    contacts['booked'] = bookings\n",
    "\n",
    "    #Length of stay\n",
    "    logging.info('Adding Feature: num_nights: Number of nights requested')\n",
    "    contacts['num_nights'] = (contacts['ds_checkout_first']-contacts['ds_checkin_first']).apply(lambda x: x.days-1)\n",
    "\n",
    "    #length of time between check-in date and inquiry date\n",
    "    logging.info('Adding Feature: inq_to_checkin: Length of time(days) between inquiry and check-in date')\n",
    "    contacts['inq_to_checkin'] = (contacts['ds_checkin_first']-contacts['ts_interaction_first']).apply(lambda x: x.days)\n",
    "\n",
    "    #get day of week for check in date (Mon=0, Tues=1 ... Sun=6)\n",
    "    logging.info('Adding Feature: checkin_day_of_week: Check-in Day of Week (Mon=0, Tues=1...Sun=6)')\n",
    "    contacts['checkin_day_of_week'] = contacts['ds_checkin_first'].apply(lambda x: x.weekday())\n",
    "    \n",
    "    \n",
    "    #Merge datasets\n",
    "    logging.info('Merge Users and Contacts Table')\n",
    "    df = contacts.merge(\n",
    "        users,\n",
    "        right_on='id_user_anon',\n",
    "        left_on='id_host_anon',\n",
    "        how='left').merge(\n",
    "                users,\n",
    "                right_on='id_user_anon',\n",
    "                left_on='id_guest_anon',\n",
    "                how='left')\n",
    "    \n",
    "    logging.info('Merge Listings and Merged Table')\n",
    "    df = df.merge(listings,on='id_listing_anon',how='left')\n",
    "    \n",
    "    #Clean column names\n",
    "    logging.info('Clean columns')\n",
    "    del df['id_user_anon_x']\n",
    "    del df['id_user_anon_y']\n",
    "    df.columns = ['id_guest_anon', 'id_host_anon', 'id_listing_anon',\n",
    "           'ts_interaction_first', 'ts_reply_at_first', 'ts_accepted_at_first',\n",
    "           'ts_booking_at', 'ds_checkin_first', 'ds_checkout_first', 'm_guests',\n",
    "           'm_interactions', 'm_first_message_length_in_characters',\n",
    "           'contact_channel_first', 'guest_user_stage_first', 'reply_inq_mins',\n",
    "           'accept_inq_mins', 'book_inq_mins', 'booked', 'num_nights',\n",
    "           'inq_to_checkin', 'checkin_day_of_week',  'host_country',\n",
    "           'host_words_in_user_profile', 'guest_country',\n",
    "           'guest_words_in_user_profile', 'room_type', 'listing_neighborhood',\n",
    "           'total_reviews']\n",
    "    \n",
    "    n_samples, n_features = df.shape\n",
    "    logging.info('Complete Data    # of Observations: %s' % (n_samples))\n",
    "    logging.info('Complete Data    # of Features: %s' % (n_features))\n",
    "    \n",
    "    #Check and drop duplicates\n",
    "    logging.info('Number of duplicate rows: %s' % (df.duplicated().sum()))\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    \n",
    "    #Check number of null or NaT values in each column\n",
    "    logging.info('Generating feature null dictionary')\n",
    "    col_null = {}\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == np.dtype('<M8[ns]'):\n",
    "            num_null = len([x for x in df[col] if x is pd.Timestamp('NaT')])\n",
    "            if num_null > 0:\n",
    "                logging.warning('%s null values found in column: %s' % (num_null, col))\n",
    "                col_null[col] = num_null\n",
    "        else:\n",
    "            num_null = df[col].isnull().sum()\n",
    "            if num_null > 0:\n",
    "                logging.warning('%s null values found in column: %s' % (num_null, col))\n",
    "                col_null[col] = num_null\n",
    "\n",
    "    #Drop nulls if safe\n",
    "    logging.info('Dropping Nulls if safe to (< 15)')\n",
    "    for col in col_null.keys():\n",
    "        if col_null[col] < 15:\n",
    "            df = df[pd.notnull(df[col])]\n",
    "            df = df.reset_index(drop=True)\n",
    "    \n",
    "    # Convert # of guests to int\n",
    "    df['m_guests'] = df['m_guests'].astype(int)\n",
    "\n",
    "    # Convert # of first message length to int\n",
    "    df['m_first_message_length_in_characters'] = df['m_first_message_length_in_characters'].astype(int)\n",
    "\n",
    "    # Convert # of total reviews to int\n",
    "    df['total_reviews'] = df['total_reviews'].astype(int)\n",
    "    \n",
    "    #Remove rows where total_reviews < 0: Check with product team to see why this is occuring\n",
    "    logging.info('Removing rows where total_reviews < 0')\n",
    "    df = df[df['total_reviews'] >= 0]\n",
    "    df = df.reset_index(drop=True)\n",
    "    \n",
    "    #create column where host replied to an inquiry\n",
    "    logging.info('Creating column where host replied to an inquiry (0,1)')\n",
    "    df['host_replied'] = np.where(df['reply_inq_mins'].isnull(),0,1)\n",
    "    \n",
    "    return df\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "def eda_plots(df):\n",
    "    \"\"\"\n",
    "    *Return: Plots of key metrics used to monitor over time\n",
    "            Univariate and Distribution plots of important features\n",
    "    data: DataFrame of cleaned+enhanced dataset\n",
    "    \"\"\"\n",
    "    #graph the percent of all airbnbs booked on any given day\n",
    "    def graph_booking(data,unique_date,dates_booked):\n",
    "        bookings_per_day = pd.concat([pd.DataFrame(dates_booked.keys()),pd.DataFrame(dates_booked.values())],axis=1)\n",
    "        bookings_per_day.columns = ['date','day_booked']\n",
    "        bookings_per_day.sort_values('date',inplace=True)\n",
    "        bookings_per_day = unique_date.merge(bookings_per_day,how='left').fillna(0)\n",
    "\n",
    "        logging.info('Number of unique listings: %s' % len(set(data['id_listing_anon'])))\n",
    "        total_listings = len(set(data['id_listing_anon']))\n",
    "\n",
    "        percent_day_booked = pd.DataFrame(bookings_per_day['day_booked']/total_listings*100)\n",
    "        percent_day_booked['date'] = unique_date\n",
    "        percent_day_booked = percent_day_booked.set_index('date')\n",
    "\n",
    "        #Plot             \n",
    "        plt.figure(figsize=(20,6))\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.plot(percent_day_booked)\n",
    "        plt.xlabel('date',fontsize=15)\n",
    "        plt.ylabel('% Listings Booked',fontsize=15)\n",
    "        plt.title('% of Total Listings Booked by Day')\n",
    "        plt.savefig('./data/percent_of_total_listings_booked.png', bbox_inches='tight')\n",
    "        plt.show()\n",
    "    \n",
    "    def graph_inquiry_bookings(data, col, unique_date, metric = None, cohort = None):\n",
    "    \n",
    "        def plot(new_user_inq,metric,cohort=None):   \n",
    "            plt.figure(figsize=(20,6))\n",
    "            plt.xticks(rotation=45)\n",
    "            plt.plot(new_user_inq)\n",
    "            plt.xlabel('date',fontsize=15)\n",
    "            plt.ylabel('# of %s' % metric,fontsize=15)\n",
    "            if cohort is not None:\n",
    "                plt.title('# %s per Day %s' % (metric,str(cohort)))\n",
    "            else:\n",
    "                plt.title('# %s per Day' % metric)\n",
    "                print ('./data/%s.png' % metric)\n",
    "                plt.savefig('./data/%s.png' % metric, bbox_inches='tight')\n",
    "            plt.show()\n",
    "\n",
    "        def process(new_user_inq,unique_date):\n",
    "            new_user_inq.reset_index(drop=True,inplace=True)\n",
    "            new_user_inq = new_user_inq.apply(lambda x: pd.Timestamp(x.date()))\n",
    "            new_user_inq = pd.DataFrame(new_user_inq.value_counts()).reset_index()\n",
    "            new_user_inq.columns = ['date','ts_interaction_first']\n",
    "            new_user_inq = unique_date.merge(new_user_inq,how='left')\n",
    "            new_user_inq.fillna(0,inplace=True)\n",
    "            new_user_inq = new_user_inq.set_index('date')\n",
    "            return new_user_inq\n",
    "\n",
    "        if cohort is not None:\n",
    "            for c in np.unique(data[cohort]):\n",
    "                new_user_inq = data[data[cohort] == c][col]\n",
    "                new_user_inq = process(new_user_inq,unique_date)\n",
    "                plot(new_user_inq,metric,cohort = c)\n",
    "        else:\n",
    "            new_user_inq = data[col]\n",
    "            new_user_inq = process(new_user_inq,unique_date)\n",
    "            plot(new_user_inq,metric)\n",
    "            \n",
    "    logging.info('Starting graph generations...')\n",
    "    \n",
    "    #Create unique date list\n",
    "    logging.info('Creating base unique dates')\n",
    "    min_checkin = df['ds_checkin_first'].min()\n",
    "    max_checkin = df['ds_checkin_first'].max()\n",
    "    unique_date = pd.date_range(start=min_checkin,end=max_checkin, freq='D')\n",
    "    unique_date = pd.DataFrame(unique_date,columns=['date'])\n",
    "    \n",
    "    #Get count of airbnbs booked on any given day\n",
    "    logging.info('Generating graph for key metric: Ratio of airbnbs booked to total potential listings on any given day.')\n",
    "    dates_booked = {}\n",
    "    #Only getting count of booked airbnbs\n",
    "    booked = df[df['booked']==1]\n",
    "    for c_in,c_out in zip(booked['ds_checkin_first'],booked['ds_checkout_first']):\n",
    "        booked_date_range = pd.date_range(start=c_in,end=c_out, freq='D')\n",
    "        #A user's checkout date availability is not restricted from another user to book starting on that date\n",
    "        for dt in booked_date_range[:-1]:\n",
    "            if dt not in dates_booked:\n",
    "                dates_booked[dt] = 1\n",
    "            else:\n",
    "                dates_booked[dt] += 1\n",
    "    \n",
    "    #Generate % total listings booked by day\n",
    "    graph_booking(df,unique_date,dates_booked)\n",
    "    #Generate # Inquiries per day\n",
    "    graph_inquiry_bookings(df,'ts_interaction_first',unique_date, metric = 'Inquiries')\n",
    "    #Generate # Bookings per day\n",
    "    graph_inquiry_bookings(df,'ts_booking_at',unique_date,metric = 'Bookings')\n",
    "    \n",
    "        \n",
    "    \n",
    "\n",
    "def prep_model(df):\n",
    "    \"\"\"\n",
    "    Create prepared model for running machine learning algorithm\n",
    "    *Return: Altered DataFrame\n",
    "    \"\"\"\n",
    "    #Begin prepping dataframe\n",
    "    logging.info('Beginning preparation to feature engineer for model purposes')\n",
    "    #Excluding contact_channel_first since it correlates too high with bookings target\n",
    "    feature_cols = [\n",
    "            'm_guests',\n",
    "            'm_interactions', \n",
    "            'm_first_message_length_in_characters',\n",
    "            #'contact_channel_first', \n",
    "            'guest_user_stage_first',\n",
    "            'booked', \n",
    "            'num_nights',\n",
    "            'inq_to_checkin',\n",
    "            'checkin_day_of_week',\n",
    "            'host_country',\n",
    "            'host_words_in_user_profile',\n",
    "            'guest_country',\n",
    "            'guest_words_in_user_profile',\n",
    "            'room_type',\n",
    "            'listing_neighborhood',\n",
    "            'total_reviews',\n",
    "            'host_replied']\n",
    "    \n",
    "    prep_df = df[feature_cols]\n",
    "    \n",
    "    cat_feature = [\n",
    "    #'contact_channel_first',\n",
    "    'guest_user_stage_first',\n",
    "    'host_country',\n",
    "    'checkin_day_of_week',\n",
    "    'guest_country',\n",
    "    'room_type',\n",
    "    'listing_neighborhood',\n",
    "    'host_replied']\n",
    "    \n",
    "    df_train = prep_df[cat_feature]\n",
    "    tar_feature = 'booked'\n",
    "    \n",
    "    #Encoder for categorical variables\n",
    "    def encoder(data, categorical_feature):\n",
    "        enc = OneHotEncoder()\n",
    "        array_sparse = enc.fit_transform(np.array(data[categorical_feature]).reshape(-1,1))\n",
    "        array = array_sparse.toarray()\n",
    "        column_names = [x.replace(\"x0_\",'{}_'.format(categorical_feature)).replace(\".0\",\"\") for x in enc.get_feature_names()]\n",
    "        return array_sparse, array, column_names\n",
    "    \n",
    "    logging.info('One Hot Encoding Cat variables...')\n",
    "    for cat in cat_feature:\n",
    "        array_sparse, array, column_names = encoder(df_train, cat)\n",
    "        for i,col in enumerate(column_names[:-1]):\n",
    "            df_train[col] = array[:,i]\n",
    "      \n",
    "    #Drop categorical features after one hot encoding\n",
    "    prep_df.drop(cat_feature, axis=1, inplace=True)\n",
    "    df_train.drop(cat_feature, axis=1, inplace=True)\n",
    "    \n",
    "    model_df = pd.concat([prep_df,df_train],axis=1)\n",
    "    \n",
    "    return model_df\n",
    "    \n",
    "    \n",
    "def run_model(model_df, test_size):\n",
    "    \"\"\"\n",
    "    Run Random Forest to predict a user's likelihood to book an airbnb.\n",
    "\n",
    "    *Return: AUC of model\n",
    "             Feature Importance sorted\n",
    "    model_df: DataFrame of fix data for model\n",
    "    test_size: float between 0.0 - 1.0\n",
    "    \"\"\"\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "    model_df.drop('booked',axis=1), model_df['booked'], test_size=test_size, random_state=42)\n",
    "\n",
    "    # Create, output AUC\n",
    "    clf = RandomForestClassifier(n_jobs=2, random_state=0)\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    auc = roc_auc_score(y_true=y_test, y_score=clf.predict(X_test))\n",
    "    print (auc)\n",
    "    \n",
    "    feat_import = {}\n",
    "    for i,x in zip(model_df.columns,clf.feature_importances_):\n",
    "        feat_import[i] = x\n",
    "    \n",
    "    sorted_x = sorted(feat_import.items(), key=operator.itemgetter(1),reverse=True)\n",
    "    \n",
    "    return auc, sorted_x\n",
    "\n",
    "\n",
    "# Main section\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
